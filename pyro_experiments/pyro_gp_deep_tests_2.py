#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
The goal of this script is to show how a simple gp can be trained and how 
successive deep gp's can be trained to learn nonnormally distributed data.
For this, do the following:
    1. Imports and definitions
    2. Generate Data
    3. Model: Standard GP
    4. Model: Deep GP
    5. Training
    6. Plots and illustrations
"""


"""
    1. Imports and definitions
"""


# i) Imports

import numpy as np
import copy
import torch
import pyro
import matplotlib.pyplot as plt


# ii) Definitions

n_time = 100
n_simu = 50
t = np.linspace(0,1,n_time)
pyro.clear_param_store()



"""
    2. Generate Data
"""


# i) Distributional data

mu = np.zeros([n_time])

d_t = 0.3
sigma = np.zeros([n_time,n_time])
cov_fun = lambda s,t : np.exp(-((s-t)/d_t)**2)
for k in range(n_time):
    for l in range(n_time):
        sigma[k,l] = cov_fun(t[k],t[l])


# ii) Simulate standard GP

x_simu = np.zeros([n_simu, n_time])
for k in range(n_simu):
    x_simu[k,:] = np.random.multivariate_normal(mu, sigma)


# iii) Nonlinear transform
nonlinearity = lambda x: np.minimum(np.exp(x),2)
x_transformed = nonlinearity(x_simu)

x_data = torch.tensor(x_transformed)



"""
    3. Model: Standard GP
"""


# i) Define model

def model_gp(obs = None):
    mu_gp = pyro.param("mu_gp", torch.ones([n_time]))
    sigma_gp = pyro.param("sigma_gp", torch.eye(n_time),
                          constraint = pyro.distributions.constraints.positive_definite)
    cov_regularizer = 1e-3*(torch.eye(n_time))
    obs_dist = pyro.distributions.MultivariateNormal(loc = mu_gp, covariance_matrix = sigma_gp + cov_regularizer)
    with pyro.plate('gp_batch_plate', size = n_simu, dim = -1):
        obs = pyro.sample("obs", obs_dist, obs = obs)
    return obs
        

# ii) Define guide

def guide_gp(obs = None):
    pass

gp_pretraining = copy.copy(model_gp().detach())



"""
    4. Model: Deep GP
"""


# i) Define support function: Map to mean

class MapToMean(pyro.nn.PyroModule):
    # Initialize the module
    def __init__(self, dim_arg, dim_hidden, dim_out):
        # Initialize instance using init method from base class
        super().__init__()
        

# ii) Define support function: Map to covariance

# Map into high dim space
class MapToCov(pyro.nn.PyroModule):
    # Initialize the module
    def __init__(self, dim_arg, dim_hidden, dim_l2):
        # Evoke by passing argument, hidden, and l2 dimensions
        # Initialize instance using init method from base class
        super().__init__()
        self.dim_arg = dim_arg
        
        # linear transforms
        self.fc_1 = torch.nn.Linear(dim_arg, dim_hidden)
        self.fc_2 = torch.nn.Linear(dim_hidden, dim_hidden)
        self.fc_3 = torch.nn.Linear(dim_hidden, dim_l2)
        # nonlinear transforms
        self.nonlinear = torch.nn.Sigmoid()
        
    def forward(self, t):
        # Define forward computation on the input data t
        # Shape the minibatch so that batch_dims are on left, argument_dims on right
        t = t.reshape([-1, n_total, self.dim_arg])
        
        # Then compute hidden units and output of nonlinear pass
        hidden_units_1 = self.nonlinear(self.fc_1(x))
        hidden_units_2 = self.nonlinear(self.fc_2(hidden_units_1))
        feature_mats = self.fc_3(hidden_units_2)
        feature_mats_T = feature_mats.permute(0,2,1)
        cov_mats = torch.bmm(feature_mats, feature_mats_T)
        return cov_mats


# ii) Define model

def model_dgp(obs = None):
    mu_latent_dgp = pyro.param("mu_latent_dgp", torch.ones([n_time]))
    sigma_dgp = pyro.param("sigma_dgp", torch.eye(n_time),
                          constraint = pyro.distributions.constraints.positive_definite)
    cov_regularizer = 1e-3*(torch.eye(n_time))
    obs_dist = pyro.distributions.MultivariateNormal(loc = mu_gp, covariance_matrix = sigma_gp + cov_regularizer)




"""
    5. Training
"""


# i) Set up training parameters

learning_rate = 0.003
num_epochs = 3000
adam_args = {"lr" : learning_rate}

# Setting up svi
optimizer = pyro.optim.Adam(adam_args)
elbo_loss = pyro.infer.Trace_ELBO()
svi = pyro.infer.SVI(model = model_gp, guide = guide_gp, optim = optimizer, loss= elbo_loss)


# ii) Execute training

train_elbo = []
for epoch in range(num_epochs):
    epoch_loss = svi.step(x_data)
    train_elbo.append(-epoch_loss)
    if epoch % 100 == 0:
        print("Standard GP; Epoch : {} train loss : {}".format(epoch, epoch_loss))

gp_posttraining = copy.copy(model_gp().detach())



"""
    6. Plots and illustrations
"""


# i) Plot the data generated by original process, simple gp, and deep gp

plt.plot(x_transformed[0:5,:].T)

# Create a figure with 5 vertically aligned subplots
fig, axes = plt.subplots(5, 1, figsize=(10, 15))

# Plot the line plots
axes[0].plot(t, x_transformed[0:5,:].T)
axes[0].set_title("Original_data")

axes[1].plot(t, gp_pretraining[0:5,:].T)
axes[1].set_title("GP_pretraining")

axes[2].plot(t, gp_posttraining[0:5,:].T)
axes[2].set_title("GP_posttraining")

# axes[3].plot(x, y4)
# axes[3].set_title("Plot 4: Damped Sine")

# axes[4].plot(x, y5)
# axes[4].set_title("Plot 5: Logarithm")

# Make layout tight
plt.tight_layout()
plt.show()



# ii) Plot distributional_parameters

mu_gp = pyro.get_param_store()['mu_gp']
sigma_gp = pyro.get_param_store()['sigma_gp']

fig, axes = plt.subplots(5, 1, figsize=(10, 15))

# Plot the line plots
axes[0].plot(t, mu_gp.detach())
axes[0].set_title("GP_mean")

axes[1].imshow(sigma_gp.detach())
axes[1].set_title("GP_covariance")


























