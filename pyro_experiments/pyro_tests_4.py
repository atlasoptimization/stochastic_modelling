""" 
The goal of this script is to perform simulation and inference using pyro.
Data is generated by a Normal distribution featuring unknown mean and then 
transformed nonlinearly to generate observations. The goal is to infer
the unknown mean given a known nonlinear transform.
For this, do the following:
    1. Definitions and imports
    2. Build stochastic model
    3. Inference
    4. Plots and illustrations
"""



"""
    1. Definitions and imports ------------------------------------------------
"""


# i) Imports

import logging
import os

import torch
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import pyro
import pyro.distributions as dist
import pyro.distributions.constraints as constraints

# %matplotlib inline
# plt.style.use('default')

# ii) Definitions

n=100
n_simu=20
t=np.linspace(0,1,n)

smoke_test = ('CI' in os.environ)
assert pyro.__version__.startswith('1.8.2')

pyro.enable_validation(True)
pyro.set_rng_seed(1)
logging.basicConfig(format='%(message)s', level=logging.INFO)



"""
    2. Build stochastic model -------------------------------------------------
"""


# i) Simulate some data using true model

mu_true = 10
sigma_true = 0
data = torch.square(torch.tensor(np.random.normal(mu_true,sigma_true,[n_simu])))


# ii) Invoke observations, latent variables, parameters, model

def f_theta(theta_1,theta_2,z):
    return (theta_1+theta_2*z)**2

def g_phi(x):
    return torch.sqrt(x)

def model(data=None):
    mu = pyro.param("mu", lambda:torch.ones(()), constraint=constraints.positive)
    sigma = pyro.param("sigma", lambda:torch.ones(()), constraint=constraints.positive)
    #z = pyro.sample('z', dist.Normal(0,1))
    
    with pyro.plate("data",n_simu):
        z=pyro.sample('z', dist.Normal(0,1))
        return pyro.sample("obs",dist.Normal(f_theta(mu,sigma,z),0.01),obs=data)

# pyro.render_model(model, model_args=(), render_distributions=True, render_params=True)



"""
    3. Inference --------------------------------------------------------------
"""


# i) Create the guide

# auto_guide = pyro.infer.autoguide.AutoNormal(model)
def auto_guide(x):
    with pyro.plate("data", x.shape[0]):
        # use the encoder to get the parameters used to define q(z|x)
        z_loc= g_phi(x)
        # sample the latent code z
        return pyro.sample("z", dist.Normal(z_loc, 0.1))

# pyro.render_model(auto_guide, model_args=(), render_params=True)

# ii) Run the optimization

adam = pyro.optim.Adam({"lr": 0.02})
elbo = pyro.infer.Trace_ELBO()
svi = pyro.infer.SVI(model, auto_guide, adam, elbo)

losses = []
for step in range(1000 if not smoke_test else 2):  # Consider running for more steps.
    loss = svi.step(data)
    losses.append(loss)
    if step % 100 == 0:
        logging.info("Elbo loss: {}".format(loss))

for name, value in pyro.get_param_store().items():
    print(name, pyro.param(name).data.cpu().numpy())

    
# iii) Sample from distribution

# with pyro.plate("samples", 1, dim=-1):
samples=auto_guide(data)



"""
    4. Plots and illustrations -----------------------------------------------
"""


# i) Plot the loss

plt.figure(figsize=(5, 2), dpi=300)
plt.plot(losses)
plt.xlabel("SVI step")
plt.ylabel("ELBO loss");


# ii) Posterior predictive distribution

ppd=pyro.infer.Predictive(model,guide=auto_guide, num_samples=1)
svi_samples=ppd(data)
svi_obs=svi_samples["obs"]

obs_1=svi_obs[0,:].detach().numpy()
# obs_2=svi_obs[1,:].detach().numpy()
# obs_3=svi_obs[2,:].detach().numpy()
fig = plt.figure(figsize=(10, 6),dpi=300)
sns.histplot(obs_1, kde=True, stat="density", label="Predictive distribution",color="blue")
# sns.histplot(obs_2, kde=True, stat="density", label="Predictive distribution",color="yellow")
# sns.histplot(obs_3, kde=True, stat="density", label="Predictive distribution",color="green")
sns.histplot(data.detach().cpu().numpy(), kde=True, stat="density", label="Observed data", color="red")
fig.suptitle("Comparison of distributions");
plt.xlabel("X value")
plt.legend()
plt.show()

# iii) Check the marginal likelihood

















