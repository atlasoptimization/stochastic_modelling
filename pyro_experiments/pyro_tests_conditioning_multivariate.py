"""
The goal of this script is to showcase how conditioning can be used to create
conditional distributions and sample from them and how this interacts and 
facilitates inference via mcmc. The test case involves sampling not only at
a named sample site but sampling a subset of the variables at a sample site.

For this, do the following:
    1. Imports and definitions
    2. Create stochastic model
    3. Condition on observation
    4. Plots and illustrations
    
Note that our procedure makes use of pyro in the following sense: We build a model
that contains and samples the joint probability distribution and links them to
the subobservations (=individual sample sites corresponding to observable and 
latent random variables). This model can be conditioned on by demanding observables
to be equal to certain values (In pyro this doesnt mean, we have the conditional
distribution, its just constraining the output). To get the conditional distribution
inference is performed; the conditional distribution is approximated by the
guide after optimization.
"""


"""
    1. Imports and definitions
"""


# i) imports

import torch
import pyro
import numpy as np
import pyro.distributions as dist
import matplotlib.pyplot as plt
import logging


# ii) Definitions

num_samples = 1000



"""    
    2. Create stochastic model
"""


# i) Fixed parameters

xy_mean = torch.zeros(2)
xy_cov = torch.tensor([[1,0.9],[0.9,1]])


# ii) Model with two sample sites

# The observation mask masks out part of a batch, not partial events, the following does not work therefore
# observation_mask = torch.tensor([True, False]) 
# def model(observations = None):
#     xy = pyro.sample('xy', dist.MultivariateNormal(xy_mean,xy_cov), obs = observations, obs_mask = observation_mask )
#     return xy
# We proceed to enact partial observations as following. We consider the multivariate
# variable xy as a latent variable and then sample its first element as an observed variable
# x that we can condition on by populating the "obs" keyword

def model(observation_x = None):
    xy = pyro.sample("xy", dist.MultivariateNormal(xy_mean,xy_cov))
    x = pyro.sample("x", dist.Normal(xy[0],0.01), obs = observation_x)
    return x, xy


# iii) Generate samples from the original

samples_original = np.zeros([num_samples,2])
for k in range(num_samples):
    samples_original[k,:] = model()[0]



"""
    3. Condition on observation
"""


# i) Build model that is conditioned on having observed x = 1

# The conditioned model prescribes the value at the "x" site to be equal to
# observation_x without altering any of the other values.
observation_x = torch.tensor(1)
conditioned_model = pyro.condition(model, data = {"x": observation_x})


# ii) Perform inference to build approximation to conditional distribution

# Variational distribution to approximate conditional is set to multivariate normal
# By construction, the var. dist. takes the conditioned_model as a blueprint.
# It has as a random variable the latent variable 'xy' of the conditioned model
# and its parameters are the mean and covariance that need to be adjusted to
# maximize the ELBO to minimize KL divergence between te var. dist. q_phi(xy) 
# and the posterior p_theta(xy|x) over "xy"
variational_distribution = pyro.infer.autoguide.AutoMultivariateNormal(conditioned_model)


# inference of parameters for conditional distribution
adam = pyro.optim.Adam({"lr": 0.01})
elbo = pyro.infer.Trace_ELBO()
svi = pyro.infer.SVI(model, variational_distribution, adam, elbo)

losses = []
for step in range(1000):  # Consider running for more steps.
    loss = svi.step(observation_x)
    losses.append(loss)
    if step % 100 == 0:
        logging.info("Elbo loss: {}".format(loss))

# print all parameters in the variational distribution
for name, value in pyro.get_param_store().items():
    print(name, pyro.param(name).data.cpu().numpy())


"""
    4. Plots and illustrations
"""


# i) Sample from the variational distribution
 
samples_conditional = np.zeros([num_samples,2])
for k in range(num_samples):
    samples_conditional[k] = variational_distribution()['xy'].detach().numpy()


# ii) Plot original dataset

plt.figure(1,dpi =300)
plt.hist2d(samples_original[:,0], samples_original[:,1])    
plt.title('The original distribution')


# iii) Plot conditional dataset

plt.figure(2,dpi =300)
plt.hist2d(samples_conditional[:,0], samples_conditional[:,1])     
plt.title('The conditional distribution')


# iv) Calculate the true conditional expectation

mean_x = xy_mean[0]
mean_y = xy_mean[1]

cov_x = xy_cov[0,0]
cov_y = xy_cov[1,1]
cov_xy = xy_cov[0,1]

cond_mean_y = mean_y + (cov_xy/cov_y)*(observation_x - mean_x)
cond_mean_y_empirical = np.mean(samples_conditional[:,1])
print(('The emprical conditional mean generated by drawing samples from the variational '\
       'distribution is {}, the true one is {}').format(cond_mean_y_empirical, cond_mean_y))


# v) Showcase training progress

plt.figure(3,dpi =300)
plt.plot(losses)
plt.title('ELBO losses')


# vi) Showcase node structure

trace = pyro.poutine.trace(model).get_trace(0.0)
trace.nodes
